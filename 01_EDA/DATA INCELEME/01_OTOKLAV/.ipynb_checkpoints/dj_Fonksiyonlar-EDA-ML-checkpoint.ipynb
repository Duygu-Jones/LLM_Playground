{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67af4fa8-30b5-4a1b-ac69-de441b716cd9",
   "metadata": {},
   "source": [
    "# KULLANILABILIR FONKSYIONLAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9a32c-64f3-4f3d-ba98-9f3317724537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import cufflinks as cf\n",
    "%matplotlib inline \n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder, LabelEncoder \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import PrecisionRecallDisplay, roc_curve, average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from yellowbrick.regressor import ResidualsPlot, PredictionError\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91c62a-8ae0-45eb-bcb9-395bd4bf098a",
   "metadata": {},
   "source": [
    "## User Defined Funcs for Summary of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d507935-618e-4daa-9fd6-faac0ce0ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== User-Defined-Function ==========================\n",
    "\n",
    "def rename_columns_by_position(df, new_column_names):\n",
    "    \"\"\"\n",
    "    DataFrame'deki sütun adlarını pozisyona göre yeniden adlandırır.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Sütun adlarını değiştirmek istediğiniz DataFrame.\n",
    "        new_column_names (list): Yeni sütun adlarının listesi.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Yeni sütun adlarıyla güncellenmiş DataFrame.\n",
    "    \"\"\"\n",
    "    # Sütun sayısının uyumlu olup olmadığını kontrol et\n",
    "    if len(new_column_names) != len(df.columns):\n",
    "        raise ValueError(\"Sütun sayısı ile yeni adların sayısı eşleşmiyor.\")\n",
    "    \n",
    "    # Yeni sütun adlarını atama\n",
    "    df.columns = new_column_names   \n",
    "    return df\n",
    "\n",
    "\n",
    "#================ Categorical Features Summary ===================\n",
    "def object_summary(df):\n",
    "    obs = df.shape[0]\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "\n",
    "    # Kategorik sütunlar için özetleme\n",
    "    object_df = df.select_dtypes(include='object')\n",
    "    \n",
    "    # Yeni bir boş DataFrame oluşturma\n",
    "    summary_df = pd.DataFrame(index=object_df.columns)\n",
    "\n",
    "    summary_df['Dtype'] = object_df.dtypes\n",
    "    summary_df['Counts'] = object_df.count()\n",
    "    summary_df['Nulls'] = object_df.isnull().sum()\n",
    "    summary_df['NullPercent'] = (object_df.isnull().sum() / obs) * 100\n",
    "    summary_df['Top'] = object_df.apply(lambda x: x.mode().iloc[0] if not x.mode().empty else '-')\n",
    "    summary_df['Frequency'] = object_df.apply(lambda x: x.value_counts().max() if not x.value_counts().empty else '-')\n",
    "    summary_df['Uniques'] = object_df.nunique()\n",
    "\n",
    "    # UniqueValues sütununu kontrol ederek ekleme (dize olarak)\n",
    "    summary_df['UniqueValues'] = object_df.apply(\n",
    "        lambda x: ', '.join(map(str, x.unique()[:10])) + '...' if x.nunique() > 10 else ', '.join(map(str, x.unique()))\n",
    "    )\n",
    "\n",
    "    # DataFrame şekli ve tekrar eden satır sayısını ekrana yazdırma\n",
    "    print(f'1. Data shape (rows, columns): {df.shape}')\n",
    "    print(f'2. Number of duplicate rows: {duplicate_count}')\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "#================ Numerical Features Summary ===================\n",
    "def numeric_summary(df):\n",
    "    obs = df.shape[0]\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "\n",
    "    # Numerik sütunlar için özetleme\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Yeni bir boş DataFrame oluşturma\n",
    "    summary_df = pd.DataFrame(index=numeric_df.columns)\n",
    "\n",
    "    summary_df['Dtype'] = numeric_df.dtypes\n",
    "    summary_df['Counts'] = numeric_df.count()\n",
    "    summary_df['Nulls'] = numeric_df.isnull().sum()\n",
    "    summary_df['NullPercent'] = (numeric_df.isnull().sum() / obs) * 100\n",
    "    summary_df['Mean'] = numeric_df.mean()\n",
    "    summary_df['Std'] = numeric_df.std()\n",
    "    summary_df['Min'] = numeric_df.min()\n",
    "    summary_df['25%'] = numeric_df.quantile(0.25)\n",
    "    summary_df['50% (Median)'] = numeric_df.median()\n",
    "    summary_df['75%'] = numeric_df.quantile(0.75)\n",
    "    summary_df['Max'] = numeric_df.max()\n",
    "\n",
    "    # DataFrame şekli ve tekrar eden satır sayısını ekrana yazdırma\n",
    "    print(f'1. Data shape (rows, columns): {df.shape}')\n",
    "    print(f'2. Number of duplicate rows: {duplicate_count}')\n",
    "    return summary_df\n",
    "    \n",
    "\n",
    "#========== Get count and percentage of values for each column =================\n",
    "def get_value_count(df, column_name):\n",
    "    \"\"\"\n",
    "    This function calculates and returns a DataFrame with the value counts and \n",
    "    their corresponding percentages for a specified column in the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    vc = df[column_name].value_counts()\n",
    "    vc_norm = df[column_name].value_counts(normalize=True)\n",
    "    \n",
    "    vc = vc.rename_axis(column_name).reset_index(name='counts')\n",
    "    vc_norm = vc_norm.rename_axis(column_name).reset_index(name='percent')\n",
    "    vc_norm['percent'] = (vc_norm['percent'] * 100).map('{:.2f}%'.format)\n",
    "    \n",
    "    df_result = pd.concat([vc[column_name], vc['counts'], vc_norm['percent']], axis=1)\n",
    "    return df_result\n",
    "\n",
    "\n",
    "#============== Checks duplicates and drops them ==========================\n",
    "\n",
    "def duplicate_values(df):\n",
    "    print(\"Duplicate check...\")\n",
    "    num_duplicates = df.duplicated(subset=None, keep='first').sum()\n",
    "    if num_duplicates > 0:\n",
    "        print(\"There are\", num_duplicates, \"duplicated observations in the dataset.\")\n",
    "        df.drop_duplicates(keep='first', inplace=True)\n",
    "        print(num_duplicates, \"duplicates were dropped!\")\n",
    "        print(\"No more duplicate rows!\")\n",
    "    else:\n",
    "        print(\"There are no duplicated observations in the dataset.\")\n",
    "\n",
    "\n",
    "\n",
    "# ========== User-Defined-Function for Missing Values ============\n",
    "def missing_values(df):\n",
    "    \"\"\"This function calculates the missing values count and their percentage in a DataFrame.\"\"\"\n",
    "\n",
    "    missing_count = df.isnull().sum()\n",
    "    value_count = df.isnull().count()\n",
    "    missing_percentage = round(missing_count / value_count * 100, 2)\n",
    "    \n",
    "    # Format the percentage as '0.00%' with % symbol\n",
    "    missing_percentage_formatted = missing_percentage.map(\"{:.2f}%\".format)\n",
    "    # Create a DataFrame to store the results\n",
    "    missing_df = pd.DataFrame({\"count\": missing_count, \"percentage\": missing_percentage_formatted}) \n",
    "    return missing_df\n",
    "\n",
    "\n",
    "# ========== Plotting Missing Values  ===========================\n",
    "def na_ratio_plot(df):\n",
    "    \"\"\"Plots the ratio of missing values for each feature and prints the count of missing values.\"\"\"\n",
    "    \n",
    "    sns.displot(df.isna().melt(value_name='Missing_data',var_name='Features')\\\n",
    "                ,y='Features',hue='Missing_data',multiple='fill',aspect=9/8)\n",
    "\n",
    "    print(df.isna().sum()[df.isna().sum()>0])\n",
    "\n",
    "    \n",
    "    \n",
    "#========== Detecting Anomalies ================================\n",
    "\n",
    "def detect_anomalies(df, column_name):\n",
    "    \"\"\"\n",
    "    Detects values with unusual (non-alphanumeric) characters in a column.\n",
    "    Returns: list: Detected unusual character values.\n",
    "    \"\"\"\n",
    "    # Get the unique values in the column\n",
    "    unique_values = df[column_name].unique()    \n",
    "    # Detect values with unusual characters (non-alphanumeric)\n",
    "    unusual_characters = [val for val in unique_values if isinstance(val, str) and not val.isalnum()]\n",
    "    \n",
    "    # Return the list as a single string with values separated by commas\n",
    "    return ', '.join(unusual_characters)\n",
    "\n",
    "\n",
    "#========== Detecting Non-Numerical Characters ===========================\n",
    "\n",
    "import re\n",
    "\n",
    "def find_non_numeric_values(df, column_name):\n",
    "    \"\"\"\n",
    "    Finds unique non-numeric values in a specified column of the DataFrame.\n",
    "    \"\"\"\n",
    "    pattern = r'\\D+'  # Pattern to match non-numeric characters\n",
    "    # Find and flatten non-numeric values, then ensure uniqueness with set\n",
    "    return set(re.findall(pattern, ' '.join(df[column_name].astype(str))))\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "#======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde79ac-7d71-4548-ac59-f2ab4efb13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== User-Defined-Function ==========================\n",
    "\n",
    "#======= Get count and percentage of values for each column ======\n",
    "def get_value_count(df, column_name):\n",
    "    \"\"\"\n",
    "    This function calculates and returns a DataFrame with the value counts and \n",
    "    their corresponding percentages for a specified column in the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    vc = df[column_name].value_counts()\n",
    "    vc_norm = df[column_name].value_counts(normalize=True)\n",
    "    \n",
    "    vc = vc.rename_axis(column_name).reset_index(name='counts')\n",
    "    vc_norm = vc_norm.rename_axis(column_name).reset_index(name='percent')\n",
    "    vc_norm['percent'] = (vc_norm['percent'] * 100).map('{:.2f}%'.format)\n",
    "    \n",
    "    df_result = pd.concat([vc[column_name], vc['counts'], vc_norm['percent']], axis=1)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "\n",
    "# ========== User-Defined-Function for Missing Values ============\n",
    "def missing_values(df):\n",
    "    \"\"\"This function calculates the missing values count and their percentage in a DataFrame.\"\"\"\n",
    "\n",
    "    missing_count = df.isnull().sum()\n",
    "    value_count = df.isnull().count()\n",
    "    missing_percentage = round(missing_count / value_count * 100, 2)\n",
    "    \n",
    "    # Format the percentage as '0.00%' with % symbol\n",
    "    missing_percentage_formatted = missing_percentage.map(\"{:.2f}%\".format)\n",
    "    # Create a DataFrame to store the results\n",
    "    missing_df = pd.DataFrame({\"count\": missing_count, \"percentage\": missing_percentage_formatted})\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "\n",
    "# ============= Compare Missing Values (Train-Test ==============\n",
    "def compare_missing_values(train, test):\n",
    "    \"\"\"\n",
    "    Compares missing values between train and test datasets, returning counts, percentages, and data types.\n",
    "    \"\"\"\n",
    "    def missing_data(df, label):\n",
    "        missing_count = df.isna().sum()[df.isna().sum() > 0]\n",
    "        total_count = len(df)\n",
    "        missing_percentage = (missing_count / total_count * 100).map(\"{:.2f}%\".format)\n",
    "        return pd.DataFrame({\n",
    "            f'{label} Missing Values': missing_count,\n",
    "            f'{label} Missing Percentage': missing_percentage,\n",
    "            f'{label} dtypes': df.dtypes[missing_count.index]\n",
    "        })\n",
    "    \n",
    "    # Get missing data for train and test\n",
    "    train_missing_df = missing_data(train, 'Train')\n",
    "    test_missing_df = missing_data(test, 'Test')\n",
    "    \n",
    "    # Concatenate the missing values side by side\n",
    "    return pd.concat([train_missing_df, test_missing_df], axis=1)\n",
    "\n",
    "\n",
    "# ========== Plotting Missing Values  ===========================\n",
    "def na_ratio_plot(df):\n",
    "    \"\"\"Plots the ratio of missing values for each feature and prints the count of missing values.\"\"\"\n",
    "    \n",
    "    sns.displot(df.isna().melt(value_name='Missing_data',var_name='Features')\\\n",
    "                ,y='Features',hue='Missing_data',multiple='fill',aspect=9/8)\n",
    "\n",
    "    print(df.isna().sum()[df.isna().sum()>0])\n",
    "\n",
    "    \n",
    "    \n",
    "#========== Detecting Anomalies ================================\n",
    "\n",
    "def detect_anomalies(df, column_name):\n",
    "    \"\"\"\n",
    "    Detects values with unusual (non-alphanumeric) characters in a column.\n",
    "    Returns: list: Detected unusual character values.\n",
    "    \"\"\"\n",
    "    # Get the unique values in the column\n",
    "    unique_values = df[column_name].unique()    \n",
    "    # Detect values with unusual characters (non-alphanumeric)\n",
    "    unusual_characters = [val for val in unique_values if isinstance(val, str) and not val.isalnum()]\n",
    "    \n",
    "    # Return the list as a single string with values separated by commas\n",
    "    return ', '.join(unusual_characters)\n",
    "\n",
    "\n",
    "#========== Detecting Non-Numerical Characters ===========================\n",
    "\n",
    "import re\n",
    "\n",
    "def find_non_numeric_values(df, column_name):\n",
    "    \"\"\"\n",
    "    Finds unique non-numeric values in a specified column of the DataFrame.\n",
    "    \"\"\"\n",
    "    pattern = r'\\D+'  # Pattern to match non-numeric characters\n",
    "    # Find and flatten non-numeric values, then ensure uniqueness with set\n",
    "    return set(re.findall(pattern, ' '.join(df[column_name].astype(str))))\n",
    "\n",
    "\n",
    "#=============== Clean_Unusual_Characters ===========================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def clean_and_convert_numeric(df, column_name):\n",
    "    \"\"\"\n",
    "    This function cleans non-numeric characters from a specified column, \n",
    "    converts the column to float, and handles negative values.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column to clean and convert.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with the cleaned and converted column.\n",
    "    \"\"\"\n",
    "    # 1. Remove non-numeric characters from the specified column\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(r'[^0-9.]', '', str(x)))\n",
    "    \n",
    "    # 2. Replace empty strings with NaN\n",
    "    df[column_name].replace('', np.nan, inplace=True)\n",
    "    \n",
    "    # 3. Convert the column to float\n",
    "    df[column_name] = df[column_name].astype(float)\n",
    "    \n",
    "    # 4. Convert negatives to positives (absolute values)\n",
    "    df[column_name] = df[column_name].abs()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============== User-Defined-Fonction ======================\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def knn_impute_column(df, column, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Impute missing values in the specified column using KNN.\n",
    "    Args: df (DataFrame), column (str), n_neighbors (int): Number of neighbors (Default is 5).\n",
    "    Returns: DataFrame: DataFrame with imputed column.\n",
    "    \"\"\"\n",
    "    # Apply KNN imputation to the specified column\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df[[column]] = imputer.fit_transform(df[[column]])\n",
    "    \n",
    "    return df\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643eb61-c00f-4208-a265-af992986ab2f",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ef3ad-55f7-47f4-b1ed-7e7bd79f9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks duplicates and drops them\n",
    "\n",
    "def duplicate_values(df):\n",
    "    print(\"Duplicate check...\")\n",
    "    num_duplicates = df.duplicated(subset=None, keep='first').sum()\n",
    "    if num_duplicates > 0:\n",
    "        print(\"There are\", num_duplicates, \"duplicated observations in the dataset.\")\n",
    "        df.drop_duplicates(keep='first', inplace=True)\n",
    "        print(num_duplicates, \"duplicates were dropped!\")\n",
    "        print(\"No more duplicate rows!\")\n",
    "    else:\n",
    "        print(\"There are no duplicated observations in the dataset.\")\n",
    "\n",
    "duplicate_values(df_otoklav_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b5fa4-9763-4d18-9a2d-70778d7e55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's observe first the unique values\n",
    "\n",
    "def get_unique_values(df):\n",
    "    \n",
    "    output_data = []\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        # If the number of unique values in the column is less than or equal to 5\n",
    "        if df.loc[:, col].nunique() <= 10:\n",
    "            # Get the unique values in the column\n",
    "            unique_values = df.loc[:, col].unique()\n",
    "            # Append the column name, number of unique values, unique values, and data type to the output data\n",
    "            output_data.append([col, df.loc[:, col].nunique(), unique_values, df.loc[:, col].dtype])\n",
    "        else:\n",
    "            # Otherwise, append only the column name, number of unique values, and data type to the output data\n",
    "            output_data.append([col, df.loc[:, col].nunique(),\"-\", df.loc[:, col].dtype])\n",
    "\n",
    "    output_df = pd.DataFrame(output_data, columns=['Column Name', 'Number of Unique Values', ' Unique Values ', 'Data Type'])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd186ee-b208-4427-8a82-2c07d0f26735",
   "metadata": {},
   "source": [
    "## Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742bda5a-ec1e-4910-ae0d-3beb4f0d53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df):\n",
    "\n",
    "    missing_count = df.isnull().sum()\n",
    "    value_count = df.isnull().count()\n",
    "    missing_percentage = round(missing_count / value_count * 100, 2)\n",
    "    missing_df = pd.DataFrame({\"count\": missing_count, \"percentage\": missing_percentage})\n",
    "    return missing_df\n",
    "\n",
    "missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a13a51-5240-4e54-a100-14beb1b2d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction for counting and normalizing values in the column\n",
    "\n",
    "def value_cnt_fonc(df, column_name):\n",
    "    vc = df[column_name].value_counts()\n",
    "    vc_norm = df[column_name].value_counts(normalize=True)\n",
    "\n",
    "    vc = vc.rename_axis(column_name).reset_index(name='counts')\n",
    "    vc_norm = vc_norm.rename_axis(column_name).reset_index(name='norm_counts')\n",
    "\n",
    "    df_result = pd.concat([vc[column_name], vc['counts'], vc_norm['norm_counts']], axis=1)\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c8f6b-1644-4982-bed3-a76616e650ed",
   "metadata": {},
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ee826-b096-4273-8781-1d80991620c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUTIONS OF CATEGORICAL FEATURES;\n",
    "\n",
    "for column in cat_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.countplot(x=column, data=df, palette='BuPu')\n",
    "    plt.title(f'Distribution of Categories {column}')\n",
    "\n",
    "    ax.bar_label(ax.containers[0])\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d490d5-90d3-48c9-8c02-3a112e8ea861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORICALS FETATURES BY TARGET (income <=50k | income >50k)\n",
    "\n",
    "for i in cat_features:\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    sns.countplot(ax=ax, data=df, x=i, hue=\"income\", palette='BuPu')\n",
    "    ax.set(ylabel='Counts', title=i)\n",
    "\n",
    "    for j in [0, 1]:\n",
    "        ax.bar_label(ax.containers[j])\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d504b7f-6a6f-451d-9a89-702f5ff964da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting categorical data for univariate analysis:\n",
    "\n",
    "cats = ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
    "\n",
    "def plotFrequency(cats):\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(20,25))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, cat in zip(axes, cats):\n",
    "        if cat == 'Survived':\n",
    "            total = float(len(train[cat]))\n",
    "        else:\n",
    "            total = float(len(all_data[cat]))\n",
    "        sns.countplot(all_data[cat], palette='plasma', ax=ax)\n",
    "        \n",
    "        \n",
    "        for p in ax.patches:\n",
    "            height = p.get_height()\n",
    "            ax.text(p.get_x() + p.get_width() / 2.,\n",
    "                    height + 10,\n",
    "                    '{:1.2f}%'.format((height / total) * 100),\n",
    "                    ha=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bab494-cb5f-4ded-be88-ecd0dcbf780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUTIONS OF NUMERICAL FEATURES;\n",
    "\n",
    "numerical_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "num_vars = len(numerical_df.columns)\n",
    "\n",
    "for i, var in enumerate(numerical_df.columns, 1):\n",
    "    plt.subplot((num_vars // 3) + 1, 3, i)\n",
    "    sns.histplot(data=df, x=var, kde=True)\n",
    "    plt.title(f'Distribution of {var}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7358fc7-7bd2-4574-a6b0-ab7fed240b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,14), dpi=200)\n",
    "for i, col in enumerate(df.columns[:-1]):\n",
    "        plt.subplot(8,2,i+1)\n",
    "        sns.kdeplot(df[col])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981af4c2-18a7-4522-931c-8ded5b274985",
   "metadata": {},
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac1260-b69f-4d9c-9948-de54b60e6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Checking Outliers on Numerical Features by the Target // whis=3\n",
    "\n",
    "index = 0\n",
    "plt.figure(figsize=(20,15))\n",
    "for feature in df.select_dtypes(include=['number']).columns:\n",
    "    if feature != \"income\":\n",
    "        index += 1\n",
    "        plt.subplot(3,3,index)\n",
    "        sns.boxplot(x='income',y=feature,data=df, whis=3, palette='BuPu') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76796d1a-0d33-48a0-94ac-af3f20c5c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) BOXPLOT the OUTLIERS\n",
    "\n",
    "# Initialize the subplot counter\n",
    "x = 0\n",
    "\n",
    "# Create a figure with specified size\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# Loop through each numerical column and create a boxplot\n",
    "for col in df.select_dtypes(include=['number']).columns:\n",
    "    x += 1\n",
    "    plt.subplot(1, 8, x)\n",
    "    sns.boxplot(data=df[col])\n",
    "    plt.title(col)\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()  # Adjust subplots to fit in the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28847b0-8620-4fcf-a93c-21651f4ba4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Function: BOXPLOT the OUTLIERS (her bir sutun icin ayri ve sutun icindeki valualari gosterir)\n",
    "\n",
    "def plot_feature_outliers(df, hue_column):\n",
    "    plt.figure(figsize=(20,30))\n",
    "    for i, col in enumerate(df.columns[:-1], 1):\n",
    "        plt.subplot(9, 2, i)\n",
    "        plt.title(f\"Distribution of {col} Data with Outliers\")\n",
    "        sns.boxplot(x=hue_column, y=col, data=df)\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_feature_outliers(df, \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9878dd-dd32-4226-aaa9-22eec67f3bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot for each column by target (texbox ile)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "features = df.columns[:-1]\n",
    "for i in features:\n",
    "    fig = px.box(df, x=i, y= 'class')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fa44c-235e-44c7-9082-667f25024f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tek grafikte sadece sutunlari boxplot gosterir texbox ile\n",
    "import cufflinks as cf  \n",
    "cf.go_offline()\n",
    "df.iloc[:,1:].iplot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f7a2d-2e3a-437e-ae6a-bbfa8547a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOX Plot çizimi--> istedigin kadar boxplot ekelyebilirsin\n",
    "\n",
    "plt.figure(figsize = (20,6))\n",
    "plt.subplot(141)\n",
    "sns.boxplot(y = \"sepal_length\", x = \"labels\", data = X, palette=\"BuPu\")\n",
    "plt.subplot(142)\n",
    "sns.boxplot(y = \"sepal_width\", x = \"labels\", data = X, palette=\"BuPu\")\n",
    "plt.subplot(143)\n",
    "sns.boxplot(y = \"petal_length\", x = \"labels\", data = X, palette=\"BuPu\")\n",
    "plt.subplot(144)\n",
    "sns.boxplot(y = \"petal_width\", x = \"labels\", data = X, palette=\"BuPu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db2697-9bba-4290-b36f-dc91c7b4bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=['number'])\n",
    "\n",
    "ncols = 3\n",
    "num_plots = len(numeric_columns.columns)\n",
    "nrows = (num_plots - 1) // ncols + 1\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5 * nrows))\n",
    "\n",
    "for i, column in enumerate(numeric_columns.columns):\n",
    "    row = i // ncols\n",
    "    col = i % ncols\n",
    "    sns.boxplot(data=numeric_columns, y=column, ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'Boxplot of {column}')\n",
    "\n",
    "for i in range(num_plots, nrows * ncols):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065c5f8-482d-4ab9-a994-5bd76d14c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier deletion\n",
    "df_num = df.select_dtypes(include='number')\n",
    "for column in df_num.columns:\n",
    "    for i in df[\"Credit_Score\"].unique():\n",
    "        selected_i = df[df[\"Credit_Score\"] == i]\n",
    "        selected_column = selected_i[column]\n",
    "        \n",
    "        std = selected_column.std()\n",
    "        mean= selected_column.mean()\n",
    "        \n",
    "        max = mean + (4 * std)\n",
    "        min =  mean - (4 * std)\n",
    "        \n",
    "        outliers = selected_column[((selected_i[column] > max) | (selected_i[column] < min))].index\n",
    "        df.drop(index=outliers, inplace=True)\n",
    "        print(column, i, outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0fd7f-e818-4572-912f-5c90c2ba55f0",
   "metadata": {},
   "source": [
    "### Cleaning Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3446e33-d55a-4a54-af09-58b20b3be675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Outliers \n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate Z-Score for each numerical column\n",
    "z_scores = np.abs(stats.zscore(df.select_dtypes(include=['number'])))\n",
    "\n",
    "# Define a threshold (commonly 3)\n",
    "threshold = 3\n",
    "\n",
    "# Identify outliers\n",
    "outliers = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "# Filter out the outliers\n",
    "df_cleaned = df[~outliers]\n",
    "\n",
    "# Update the all Dataset if needed\n",
    "#df = df_cleaned.copy() \n",
    "\n",
    "# Display the shape of the dataframe before and after outlier removal\n",
    "print(\"Original dataframe shape:\", df.shape)\n",
    "print(\"Dataframe shape after outlier removal:\", df.shape)\n",
    " \n",
    "\n",
    "# Display the first few rows of the cleaned dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5e289-75de-4bb1-9996-0ba39f67db5b",
   "metadata": {},
   "source": [
    "## Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e78d2-0689-4f9d-8cc3-714476317ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate skewness for numeric features\n",
    "\n",
    "# A skewness value greater than 1 indicates positive skewness,\n",
    "# a skewness value less than -1 indicates negative skewness,\n",
    "# and a skewness value close to zero indicates a relatively symmetric distribution.\n",
    "\n",
    "num_cols= df.select_dtypes('number').columns\n",
    "\n",
    "skew_limit = 0.75               # define a limit above which we will log transform\n",
    "skew_vals = df[num_cols].skew()\n",
    "\n",
    "\n",
    "# Showing the skewed columns\n",
    "skew_cols = (skew_vals\n",
    "             .sort_values(ascending=False)\n",
    "             .to_frame()\n",
    "             .rename(columns={0:'Skew'})\n",
    "             .query('abs(Skew) > {}'.format(skew_limit)))\n",
    "skew_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0adc023-23ca-4796-a001-314662de252f",
   "metadata": {},
   "source": [
    "###  Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8e397-cbaf-4fb2-82ad-8e6f3b6998bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to skewed columns\n",
    "\n",
    "for col in skew_cols.index:\n",
    "    # Since log transformation cannot be applied to non-positive values, we add 1 to each value\n",
    "    df[col] = np.log1p(df[col])\n",
    "\n",
    "# Display the transformed dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465f066-3ebe-4fd2-82e3-6681e4255d12",
   "metadata": {},
   "source": [
    "## ANOVA Test for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc30a3-d237-41f5-b4b8-707ed3693510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANOVA test for each categorical feature\n",
    "anova_results = {}\n",
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for feature in categorical_features:\n",
    "    groups = [df[\"co2_emissions\"][df[feature] == category].values for category in df[feature].unique()]\n",
    "    anova_results[feature] = stats.f_oneway(*groups)\n",
    "\n",
    "# Display the ANOVA results\n",
    "for feature, result in anova_results.items():\n",
    "    print(f\"ANOVA result for {feature}:\")\n",
    "    print(f\"F-statistic: {result.statistic}, p-value: {result.pvalue}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129f3a0-aa46-4729-af39-b85248268e20",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0367bf-e772-486f-92ce-72125e885dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Defined-Functions\n",
    "#####################################################################################\n",
    "\n",
    "# Function to Evaluate the Model Performans using Classification Confusion_matrix() \n",
    "# Also does the prediction in the function\n",
    "\n",
    "def eval_metric(model, X_train, y_train, X_test, y_test, i):\n",
    "\n",
    "    \"\"\" to get the metrics for the model \"\"\"\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"{i} Test_Set\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print()\n",
    "    print(f\"{i} Train_Set\")\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "    \n",
    "#####################################################################################\n",
    "\n",
    "# Function to display Feature Importance\n",
    "def plot_feature_importance(model, X_train, figsize=(8, 5)):\n",
    "    \"\"\"\n",
    "    Plots the feature importances of a fitted model as a horizontal bar plot,\n",
    "    with the importance values displayed next to the bars.\n",
    "    \"\"\"\n",
    "    # Get feature importances\n",
    "    feature_importances = model.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feats = pd.Series(data=feature_importances, index=X_train.columns).sort_values(ascending=False)\n",
    "    \n",
    "    # Plot the feature importances as a horizontal bar plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(y=feats.index, x=feats.values, orient='h', palette='Blues')\n",
    "\n",
    "    # Add the importance values next to the bars\n",
    "    for index, value in enumerate(feats.values):\n",
    "        plt.text(value, index, f'{value:.2f}', va='center',fontsize=10)\n",
    "\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.show()\n",
    "#####################################################################################\n",
    "\n",
    "# Function to display Feature Importance\n",
    "def plot_feature_importance(model, X_train, figsize=(8, 5)):\n",
    "    \"\"\"\n",
    "    Plots the feature importances of a fitted model as a horizontal bar plot.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame of sorted feature importances.\n",
    "    \"\"\"\n",
    "    # Get feature importances\n",
    "    feature_importances = model.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feats = pd.DataFrame(data=feature_importances, index=X_train.columns, columns=['importance'])\n",
    "    feats = feats.sort_values(\"importance\", ascending=False)\n",
    "    \n",
    "    # Plot the feature importances as a horizontal bar plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(data=feats, y=feats.index, x='importance', orient='h', palette='Blues')\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.show()\n",
    "    \n",
    "    return feats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
