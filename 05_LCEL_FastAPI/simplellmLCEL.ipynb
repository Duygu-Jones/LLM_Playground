{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Simple LLM Application with LCEL(Langchain Expression Language)\n",
    "\n",
    "In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. \n",
    "This is a relatively simple LLM application - it's just a single LLM call plus some prompting. \n",
    "Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\n",
    "\n",
    "In this Notebook:\n",
    "\n",
    "- Using language models\n",
    "- Using PromptTemplates and OutputParsers\n",
    "- Using LangChain Expression Language (LCEL) to chain components together\n",
    "- Debugging and tracing your application using LangSmith\n",
    "- Deploying your application with LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq \n",
    "\n",
    "### Language Processing Unit Groq builds fast AI inference. \n",
    "\n",
    "The Groq LPU™, AI Inference Technology, delivers exceptional compute speed, affordability, and energy efficiency at scale.  Groq solutions are based on the Language Processing Unit (LPU), a new category of processor. Groq is the creator of the LPU and built it from the ground up to meet the unique characteristics and needs of AI. LPUs run Large Language Models (LLMs) at substantially faster speeds and, on an architectural level, up to 10x better energy efficiency compared to GPUs.\n",
    "\n",
    "This paper explains the design principles of the Groq LPU and why its architecture delivers such exceptional performance.\n",
    "chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://groq.com/wp-content/uploads/2024/07/GroqThoughts_WhatIsALPU-vF.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Environment Setup: Loads API keys from environment variables for OpenAI and Groq services.\n",
    "    - Create a text file named `.env` in your project root folder and add your API keys like:\n",
    "        ```\n",
    "        OPENAI_API_KEY= \"your-key-here\"\n",
    "        GROQ_API_KEY= \"your-key-here\"\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file and initialize OpenAI and Groq clients\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LangSmith Configuration: Sets up environment variables for tracking and monitoring LangChain operations in LangSmith platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LangSmith tracking with API key and project settings\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Initialization: Sets up a chat model using Groq's Gemma-2-9b-it model with the specified API key.\n",
    "    - ChatGroq: A LangChain integration that allows using Groq's language models through their API, providing access to models like Gemma-2-9b for chat completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000222AE1B8CE0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000222AE1B9340>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize chat models from OpenAI and Groq providers\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Message Chain Creation: Sets up a conversation with system and user messages for language translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create message chain for translation and invoke the model\n",
    "\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"Translate the following from English to French\"),\n",
    "    HumanMessage(content=\"Hello How are you?\")\n",
    "]\n",
    "\n",
    "result=model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour, comment allez-vous ? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 21, 'total_tokens': 32, 'completion_time': 0.02, 'prompt_time': 0.000202349, 'queue_time': 0.013323771000000002, 'total_time': 0.020202349}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-748a5716-3dd5-4afa-8f20-2d2d33874c34-0', usage_metadata={'input_tokens': 21, 'output_tokens': 11, 'total_tokens': 32})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output Parsing: Converts the model's response into a plain string format for easier handling.\n",
    "    - StrOutputParser: A simple parser in LangChain that converts model outputs into plain text strings, removing any special formatting or metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, comment allez-vous ? \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse model output to string format\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser=StrOutputParser()\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LCEL Pipeline: Creates a simple chain by combining the model and parser using LangChain Expression Language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are a few ways to say \"Hello, how are you?\" in French:\\n\\n**Formal:**\\n\\n* **Bonjour, comment allez-vous ?** (This is the most formal option and suitable for addressing someone you don\\'t know well or someone older than you.)\\n\\n**Informal:**\\n\\n* **Salut, comment vas-tu ?** (This is a more casual greeting, suitable for friends and family.)\\n* **Coucou, comment ça va ?** (This is a very informal greeting, often used with close friends.)\\n\\nLet me know if you\\'d like more options or have any other phrases you\\'d like translated!\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and execute a simple LCEL chain combining model and parser\n",
    "\n",
    "chain=model|parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt Template Creation: Sets up a reusable chat prompt structure for translation tasks with language and text variables.\n",
    "    - ChatPromptTemplate: A LangChain class that creates structured templates for chat interactions, allowing dynamic input of variables like language and text into predefined message formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flexible prompt template for translations\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generic_template=\"Translate the following into {language}:\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",generic_template),(\"user\",\"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into French:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=prompt.invoke({\"language\":\"French\",\"text\":\"Hello\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into French:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LCEL Chain Construction: Combines prompt template, model, and parser into a single pipeline for streamlined translation tasks.\n",
    "    - LCEL Chain: A complete processing pipeline that strings together multiple components (prompt template, model, and parser) using the pipe operator to handle translation requests in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create full translation chain: prompt -> model -> parser\n",
    "\n",
    "chain=prompt|model|parser\n",
    "chain.invoke({\"language\":\"French\",\"text\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **LCEL Components Guide**: overview of components that can be added to LangChain pipelines for enhanced functionality.\n",
    "\n",
    "* **Main Components**:\n",
    "\n",
    "1. **Retriever**\n",
    "   - Purpose: Finds relevant documents from knowledge base for context-based responses\n",
    "   - Usage: `chain = retriever | prompt | model | parser`\n",
    "\n",
    "2. **Advanced Output Parsers**\n",
    "   - Purpose: Converts responses into specific formats (JSON, tables)\n",
    "   - Usage: `parser = StructuredOutputParser()`\n",
    "\n",
    "3. **Memory**\n",
    "   - Purpose: Stores conversation history and past interactions\n",
    "   - Usage: `chain = memory | retriever | prompt | model | parser`\n",
    "\n",
    "4. **Tools**\n",
    "   - Purpose: Enables model to access external tools (calculators, APIs)\n",
    "   - Usage: `chain = tool | prompt | model | parser`\n",
    "\n",
    "5. **Subchains**\n",
    "   - Purpose: Combines multiple chains for complex workflows\n",
    "   - Usage: `main_chain = chain_1 | chain_2`\n",
    "\n",
    "6. **Callback Handlers**\n",
    "   - Purpose: Monitors and logs model responses\n",
    "   - Usage: `chain = prompt | model | parser | callback`\n",
    "\n",
    "7. **Error Handlers**\n",
    "   - Purpose: Manages and catches errors in the chain\n",
    "   - Usage: `chain = error_handler | prompt | model | parser`\n",
    "\n",
    "8. **Post-Processing**\n",
    "   - Purpose: Modifies or analyzes model outputs\n",
    "   - Usage: `chain = prompt | model | parser | post_process`\n",
    "\n",
    "* **Key Benefit**: These components can be combined to create customized workflows based on your application's needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
