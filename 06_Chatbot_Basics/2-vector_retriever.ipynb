{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Vector Stores and Retrievers in LangChain](#toc1_)    \n",
    "  - [Documents](#toc1_1_)    \n",
    "  - [Retrievers in LangChain](#toc1_2_)    \n",
    "  - [RAG Chain](#toc1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Vector Stores and Retrievers in LangChain](#toc0_)\n",
    "\n",
    "LangChain helps computers understand and find information quickly using special tools called vector stores and retrievers. These tools are like smart libraries that help language models find the right information fast.\n",
    "\n",
    "What You'll Learn:\n",
    "- What documents are\n",
    "- How vector stores save information \n",
    "- How retrievers find exactly what you need\n",
    "\n",
    "Main Goals:\n",
    "- Help AI get the right information\n",
    "- Make search results more accurate\n",
    "- Allow computers to understand context better\n",
    "\n",
    "Think of it like a super-smart search engine that understands not just words, but the meaning behind them. Instead of just matching keywords, these tools can find information that really matches what you're looking for.\n",
    "\n",
    "We'll explore how these cool technologies work together to make AI smarter and more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-chroma\n",
    "%pip install langchain_groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Documents](#toc0_)\n",
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has two attributes:\n",
    "\n",
    "- page_content: a string representing the content;\n",
    "- metadata: a dict containing arbitrary metadata.\n",
    "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\n",
    "\n",
    "Let's generate some sample documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "        metadata={\"source\": \"bird-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(metadata={'source': 'fish-pets-doc'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.'),\n",
       " Document(metadata={'source': 'bird-pets-doc'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Rabbits are social animals that need plenty of space to hop around.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- API Key and Environment Setup: Configures environment variables and initializes the Groq language model using Llama3-8b.\n",
    "- Model Initialization: Prepares the ChatGroq model for interaction using API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B791236BD0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B7912375F0>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Set env\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Set the model\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model=\"Llama3-8b-8192\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding Model Setup: Configures an open-source embedding model from Hugging Face for converting text into vector representations.\n",
    "- Vectorization Preparation: Initializes the \"all-MiniLM-L6-v2\" model for efficient text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for Hugging Face embedding\n",
    "import tqdm as notebook_tqdm\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize a lightweight, open-source HuggingFace embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chroma Vector Store Initialization: Creates a vector database using ChromaDB with the previously configured HuggingFace embeddings.\n",
    "- Document Vectorization: Converts documents into a searchable vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1b7b702e330>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Chroma vector store from LangChain\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create a vector store by embedding the documents using the Hugging Face embeddings\n",
    "vectorstore = Chroma.from_documents(documents, embedding=embeddings)\n",
    "\n",
    "# Display the vector store configuration\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conduct a semantic similarity search for \"cat\"\n",
    "vectorstore.similarity_search(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Asynchronous Semantic Search: Performs an asynchronous similarity search for \"cat\" in the vector store.\n",
    "- Non-Blocking Retrieval: Enables concurrent document retrieval without blocking the main execution thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asynchronous semantic similarity search for \"cat\"\n",
    "\n",
    "await vectorstore.asimilarity_search(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       "  0.9351056814193726),\n",
       " (Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       "  0.9351056814193726),\n",
       " (Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       "  1.5740896463394165),\n",
       " (Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       "  1.5740896463394165)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity search with relevance scoring for \"cat\"\n",
    "vectorstore.similarity_search_with_score(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Retrievers in LangChain](#toc0_)\n",
    "\n",
    "LangChain VectorStore objects cannot be directly integrated into Expression Language chains because they do not inherit from the Runnable class.\n",
    "\n",
    "LangChain Retrievers are Runnables that implement standard methods (synchronous, asynchronous, and batch operations) and are designed for LCEL chain integration.\n",
    "\n",
    "You can create a simple Runnable without subclassing Retriever by choosing a document retrieval method like similarity_search.\n",
    "\n",
    "Retrievers are preferred in LangChain because they:\n",
    "1. Directly integrate into LCEL chains\n",
    "2. Support flexible retrieval methods\n",
    "3. Enable seamless chained operations\n",
    "\n",
    "This approach provides an easy, standardized way to retrieve and use documents in AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Custom Retriever Creation: Builds a custom Runnable retriever using RunnableLambda with the vectorstore's similarity_search method.\n",
    "- Batch Retrieval: Demonstrates retrieving documents for multiple queries simultaneously.\n",
    "\n",
    "    - Uses RunnableLambda to wrap vectorstore's similarity_searc\n",
    "    - Sets a fixed number of results per query (k=1)\n",
    "    - Enables batch retrieval for multiple search terms\n",
    "    - Converts vectorstore into a Runnable for LCEL integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')],\n",
       " [Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary type hints and LangChain classes\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create a custom retriever with a fixed number of results (k=1)\n",
    "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)\n",
    "\n",
    "# Perform batch retrieval for multiple queries\n",
    "retriever.batch([\"cat\", \"dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vectorstores: Vectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')],\n",
       " [Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # Use similarity-based search\n",
    "    search_kwargs={\"k\": 1}     # Limit to 1 result per query\n",
    ")\n",
    "\n",
    "# Perform batch retrieval for multiple queries\n",
    "retriever.batch([\"cat\", \"dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[RAG Chain](#toc0_)\n",
    "\n",
    "- RAG Chain Setup: Creates a Retrieval-Augmented Generation (RAG) workflow that combines document retrieval with language model response generation.\n",
    "- Context-Aware Question Answering: Builds a system that uses retrieved documents to inform the model's answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, dogs are great companions, known for their loyalty and friendliness.\n"
     ]
    }
   ],
   "source": [
    "## RAG\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define a template for context-based answering\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "# Create a prompt template from the message\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "# Construct the RAG chain\n",
    "rag_chain = {\n",
    "    \"context\": retriever,        # Retrieve relevant documents\n",
    "    \"question\": RunnablePassthrough()  # Pass the original question\n",
    "} | prompt | llm  # Add prompt template and language model\n",
    "\n",
    "# Invoke the chain with a question\n",
    "response = rag_chain.invoke(\"tell me about dogs\")\n",
    "\n",
    "# Print the model's response\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`message`**\n",
    "   - Defines a message template to generate a response to the user's question using only the provided context.\n",
    "\n",
    "2. **`ChatPromptTemplate.from_messages()`**\n",
    "   - Converts the message template into a **ChatPromptTemplate** object.\n",
    "   - **`{question}`** and **`{context}`** placeholders are dynamically filled.\n",
    "\n",
    "3. **`RunnablePassthrough()`**\n",
    "   - **Purpose:** Passes the input through without processing.\n",
    "   - Here, it directly transfers the value corresponding to the `\"question\"` key to the chain.\n",
    "\n",
    "4. **`rag_chain`**\n",
    "   - **Chaining Process:**\n",
    "     - **`\"context\": retriever`**: Retriever obtains the context and provides necessary information for the response.\n",
    "     - **`\"question\": RunnablePassthrough()`**: The question is added to the chain without modification.\n",
    "     - **`| prompt | llm`**: Response is generated from the model based on context and question.\n",
    "\n",
    "5. **`rag_chain.invoke(\"tell me about dogs\")`**\n",
    "   - **Input:** `\"tell me about dogs\"`.\n",
    "   - Chain:\n",
    "     - **Retriever:** Fetches context relevant to \"dogs\" query.\n",
    "     - **Prompt:** Transforms question and context into model input.\n",
    "     - **LLM:** Produces response.\n",
    "\n",
    "6. **`response.content`**\n",
    "   - **Purpose:** Returns the text content of the response generated by the model.\n",
    "\n",
    "**Result:** A response is generated using only the context for the user's \"tell me about dogs\" question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
