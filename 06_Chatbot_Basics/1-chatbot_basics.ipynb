{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Building A Basic Chatbot](#toc1_)    \n",
    "  - [Message History](#toc1_1_)    \n",
    "  - [Prompt templates](#toc1_2_)    \n",
    "  - [Managing the Conversation History](#toc1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Building A Basic Chatbot](#toc0_)\n",
    "\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "Note that this chatbot that we build will only use the language model to have a conversation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Environment Setup: Loads environment variables and retrieves Groq API key.\n",
    "    * **Environment Variables**: A method to securely store and access sensitive information like API keys using a `.env` file and the `python-dotenv` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Loading all the environment variables\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Initialization: Creates a ChatGroq instance using Gemma-2-9b model with the specified API key.\n",
    "    - ChatGroq: A LangChain integration class that provides access to Groq's language models through their API, specifically configured here to use the Gemma-2-9b-It model for text generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002B4B92EBBF0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002B4B918F740>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import ChatGroq for accessing Groq's language models\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize ChatGroq with Gemma model and API key\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Direct Model Interaction: Sends a message to the Groq model to test basic communication.\n",
    "- HumanMessage: A LangChain message type that formats user input for language model interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Duygu, it's a pleasure to meet you!\\n\\nAs a Chief AI Engineer, I imagine you have a lot of exciting projects and challenges on your plate.  \\n\\nWhat kind of work are you currently focused on? Perhaps I can be of assistance with some AI-related tasks or offer insights based on my knowledge.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 23, 'total_tokens': 93, 'completion_time': 0.127272727, 'prompt_time': 0.00014234, 'queue_time': 0.014502727, 'total_time': 0.127415067}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ec014a83-5cf1-43b5-8ea8-d7bf38fa1eea-0', usage_metadata={'input_tokens': 23, 'output_tokens': 70, 'total_tokens': 93})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model with a simple human message\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is Duygu and I am a Chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Message Chain Testing: Tests model's ability to maintain context through a series of messages.\n",
    "* **Conversation Context**: Demonstrates how to create a conversation chain using both HumanMessage and AIMessage objects to test the model's memory and context understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Duygu, and you are a Chief AI Engineer.  \\n\\nIs there anything else you'd like to tell me about yourself or your work? üòä \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 99, 'total_tokens': 136, 'completion_time': 0.067272727, 'prompt_time': 0.003552545, 'queue_time': 0.011057773, 'total_time': 0.070825272}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-8f0a3d46-920a-40a3-8124-94083cf7d4c7-0', usage_metadata={'input_tokens': 99, 'output_tokens': 37, 'total_tokens': 136})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model's context memory with a conversation sequence\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Duygu and I am a Chief AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Duygu! It's nice to meet you. \\n\\nAs a Chief AI Engineer, what kind of projects are you working on these days? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI.\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Message History](#toc0_)\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chat History Implementation: Sets up a system to maintain conversation history across different sessions.\n",
    "- Chat History System: A mechanism that stores and manages conversation history using LangChain's message history components, enabling the model to maintain context across multiple interactions within the same session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up chat history management system\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create storage for different chat sessions\n",
    "store = {}\n",
    "\n",
    "# Function to get or create chat history for a session\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Add message history capability to the model\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Chat Session Management**: Explains how to handle conversation history for different chat sessions.\n",
    "\n",
    "**Main Components**:\n",
    "1. **get_session_history Function**:\n",
    "   - Takes a session_id as input\n",
    "   - Returns a chat history object for that session\n",
    "   - Creates new history if session doesn't exist\n",
    "   - Type hint `-> BaseChatMessageHistory` indicates return type\n",
    "\n",
    "2. **Storage System**:\n",
    "   - `store`: Dictionary that keeps chat histories\n",
    "   - Key: session_id\n",
    "   - Value: ChatMessageHistory object\n",
    "\n",
    "3. **History Integration**:\n",
    "   - `RunnableWithMessageHistory`: Combines model with chat history\n",
    "   - Enables context-aware responses\n",
    "   - Maintains separate history for each session\n",
    "\n",
    "This system allows the model to remember past conversations within each unique session, making responses more contextually relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up config with session ID for message history tracking\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration Setup: Defines session configuration for chat history tracking.\n",
    "- Session Configuration: A configuration object that specifies the session ID (\"chat1\") for tracking conversation history, allowing the system to maintain context for this specific chat session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send message to model with history tracking enabled\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Duygu and I am a Chief AI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Message Invocation: Sends a message to the model while tracking conversation history.\n",
    "- Historical Context: Invokes the model with message history enabled, using the specified session configuration to maintain conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Duygu,\\n\\nIt's nice to meet you! Being a Chief AI Engineer is a fascinating role. What kind of projects are you currently working on? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI.  \\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Context Testing: Tests the model's ability to recall information from previous messages in the conversation.\n",
    "- Memory Verification: Asks the model to demonstrate its memory of earlier conversation by recalling user's name from chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Duygu.  üòä \\n\\nI remember that from our introduction!  \\n\\n\\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 96, 'total_tokens': 128, 'completion_time': 0.058181818, 'prompt_time': 0.004082993, 'queue_time': 0.010591244, 'total_time': 0.062264811}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-773c205e-8a7f-43b7-9fd8-20c7daa410d7-0', usage_metadata={'input_tokens': 96, 'output_tokens': 32, 'total_tokens': 128})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model's memory by asking about previously mentioned information\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Session Switch Test: Creates a new session to test memory isolation between different conversations.\n",
    "- Context Separation: Demonstrates how different session IDs maintain separate conversation histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'd be happy to know! üòä\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test memory isolation with a new session ID\n",
    "config2={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi John, it's nice to meet you!  üëã  \\n\\nWhat can I do for you today?\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize new session with different user information\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is John, remember?  üòä  \\n\\nIs there anything else you'd like to talk about?\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify memory retention in chat2 session\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Prompt templates](#toc0_)\n",
    "- Prompt Templates help to turn raw user information into a format that the LLM can work with. \n",
    "- In this case, the raw user input is just a message, which we are passing to the LLM. \n",
    "    - First, add in a system message with some custom instructions (but still taking messages as input). \n",
    "    - Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt Template Creation: Configures a flexible ChatPromptTemplate with a system message and dynamic message placeholder.\n",
    "- Chain Composition: Combines the prompt template with a language model to create a conversational processing chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes for creating prompt templates\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create a prompt template with system instruction and message history support\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Amnswer all the question to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compose the chain by combining the prompt template with the model\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Duygu! It's nice to meet you. \\n\\nI'm glad you're here. What can I help you with today? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 32, 'total_tokens': 69, 'completion_time': 0.067272727, 'prompt_time': 0.000307939, 'queue_time': 0.015237101, 'total_time': 0.067580666}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e2124d0e-b5a3-4377-8112-f9b88e1c914f-0', usage_metadata={'input_tokens': 32, 'output_tokens': 37, 'total_tokens': 69})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with an initial human message\n",
    "\n",
    "chain.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Hi My name is Duygu\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Message History Configuration**: Sets up a runnable component that enables persistent message history tracking across conversation sessions.\n",
    "* **Session History Management**: Integrates a mechanism to retrieve and store conversation history using the `get_session_history` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RunnableWithMessageHistory wrapper \n",
    "# This allows maintaining conversation context across multiple interactions\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,                  # Base conversational chain\n",
    "    get_session_history     # Function to retrieve session-specific message history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Session Configuration: Creates a configuration for a specific chat session with a unique session ID.\n",
    "- Message Invocation: Sends a message to the chain with session-specific configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Duygu! üëã \\n\\nIt's nice to meet you.  What can I do for you? \\n\\nI'm ready to answer your questions and help in any way I can. üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 32, 'total_tokens': 79, 'completion_time': 0.085454545, 'prompt_time': 0.000304458, 'queue_time': 0.013443822, 'total_time': 0.085759003}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-898ad635-9de9-4db0-80f2-c004650227cb-0', usage_metadata={'input_tokens': 32, 'output_tokens': 47, 'total_tokens': 79})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure a unique session identifier for the conversation\n",
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "\n",
    "# Invoke the chain with message history, using the specified session configuration\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Duygu\")],  # Initial message from the user\n",
    "    config=config  # Session-specific configuration\n",
    ")\n",
    "\n",
    "# Display the response from the model\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Duygu! üòä I remember that you told me. \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Session Context Verification:\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Merhaba Duygu!  Tanƒ±≈ütƒ±ƒüƒ±ma memnun oldum.  Nasƒ±l yardƒ±mcƒ± olabilirim? üòä \\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Session Context Verification\n",
    "\n",
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Duygu\")],\n",
    "                       \"language\":\"Turkish\"})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour Duygu, enchant√© de te rencontrer ! Comment puis-je t'aider ? üòä  \\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am Duygu\")],\n",
    "     \"language\":\"French\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \n",
    "     \"language\": \"French\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ton nom est Duygu. üòä  \\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Managing the Conversation History](#toc0_)\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Message Trimming Configuration: Creates a message trimmer to manage conversation history and control token length.\n",
    "- History Management Strategy: Configures a mechanism to limit and control the conversation context based on specific parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes for message manipulation\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "# Create a message trimmer with specific configuration parameters\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,            # Maximum number of tokens allowed in the context\n",
    "    strategy=\"last\",          # Keep the most recent messages\n",
    "    token_counter=model,      # Use the model's token counting method\n",
    "    include_system=True,      # Include system messages in the trimming process\n",
    "    allow_partial=False,      # Disallow partial message inclusion\n",
    "    start_on=\"human\"          # Begin trimming from the last human message\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates building a flexible conversational AI pipeline using LangChain, enabling dynamic message history management and context retention across different chat sessions with intelligent message trimming and session-based memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duygu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\duygu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\duygu\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of messages representing a conversation history\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),  # System instruction\n",
    "    HumanMessage(content=\"hi! I'm bob\"),               # First human message\n",
    "    AIMessage(content=\"hi!\"),                          # AI response\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),  # Another human message\n",
    "    AIMessage(content=\"nice\"),                         # AI response\n",
    "    HumanMessage(content=\"whats 2 + 2\"),               # Math question\n",
    "    AIMessage(content=\"4\"),                            # Math answer\n",
    "    HumanMessage(content=\"thanks\"),                    # Gratitude\n",
    "    AIMessage(content=\"no problem!\"),                  # AI acknowledgment\n",
    "    HumanMessage(content=\"having fun?\"),               # Casual question\n",
    "    AIMessage(content=\"yes!\"),                         # AI response\n",
    "]\n",
    "\n",
    "# Apply the trimmer to the message list\n",
    "# This will reduce the messages based on the previously defined trimming strategy\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chain Composition: Constructs a complex runnable pipeline that dynamically trims messages before processing.\n",
    "- Message Processing: Combines message history trimming, prompt generation, and model inference in a single workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I don't have access to your personal information, including your ice cream preferences.  \\n\\nWhat's your favorite flavor?  üç¶\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary utilities for function composition\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create a sophisticated chain that:\n",
    "# 1. Passes through existing messages\n",
    "# 2. Trims messages using the previously defined trimmer\n",
    "# 3. Applies the prompt template\n",
    "# 4. Generates model response\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer) | prompt | model\n",
    ")\n",
    "\n",
    "\n",
    "# Invoke the chain with:\n",
    "# - Existing message history\n",
    "# - A new human message \n",
    "# - Additional context (language)\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve and display the model's response content\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You asked what 2 + 2 equals. üòä  \\n\\n\\n\\nLet me know if you'd like to try another one!\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content\n",
    "\n",
    "# chat4 ten itibaren gecmis konusmalari hatirlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the chain with a query about a previous conversation detail\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],  # Add new query to existing messages\n",
    "        \"language\": \"English\",  # Specify language context\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the model's response content\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Session Isolation Verification: Tests memory reset by starting a new chat session with a different session ID.\n",
    "- Context Loss Demonstration: Verifies that the new session does not retain memory from previous conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to past conversations or any personal information about you, including your name. \\n\\nIs there anything else I can help you with?\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure a new session with a unique identifier\n",
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}  # New chat session\n",
    "\n",
    "# Invoke the message history chain with the new session configuration\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],  # Query about name\n",
    "        \"language\": \"English\",  # Language context\n",
    "    },\n",
    "    config=config,  # Use the new session configuration\n",
    ")\n",
    "\n",
    "# Retrieve and display the model's response\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a helpful assistant, I have no memory of past conversations. Could you please tell me the math problem you'd like me to help with? üòÑ  \\n\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the message history chain in the new chat5 session\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],  # Query about past interaction\n",
    "        \"language\": \"English\",  # Language context\n",
    "    },\n",
    "    config=config  # Use the new chat5 session configuration\n",
    ")\n",
    "\n",
    "# Retrieve and display the model's response\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
